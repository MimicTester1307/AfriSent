{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-xQxra_D2En"
      },
      "source": [
        "# Necessary Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJRi14TofUNt"
      },
      "outputs": [],
      "source": [
        "%pip install transformers --quiet\n",
        "%pip install sentencepiece --quiet  # required dependency for AfriBERTA model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sUyC91JQDfjS"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Set device to GPU or CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dai7LI6cqBgj"
      },
      "source": [
        "# Setup data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UmiBb8_SqHpv",
        "outputId": "67df7235-fae1-4c25-f477-277e45fda1db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'afrisent-semeval-2023' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "%git clone https://github.com/afrisenti-semeval/afrisent-semeval-2023.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYNlNrJymT2y"
      },
      "source": [
        "# Custom Dataset class\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qU0LI_MymSYI"
      },
      "outputs": [],
      "source": [
        "class TweetDataset(Dataset):\n",
        "  def __init__(self, file_path, tokenizer):\n",
        "    self.tokenizer = tokenizer\n",
        "    self.data = pd.read_csv(file_path, sep='\\t')\n",
        "\n",
        "    # transform the labels into binary integers (0 or 1), using custom-defined function\n",
        "    self.data[\"label\"] = self.data[\"label\"].apply(lambda x: self._convert_label(x))\n",
        "\n",
        "\n",
        "  def _convert_label(self, sentiment):\n",
        "    return 0 if sentiment == \"negative\" else 1\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    tweet = str(self.data.loc[idx, 'tweet'])\n",
        "    label = self.data.loc[idx, 'label']\n",
        "\n",
        "    # Tokenize the tweet\n",
        "    encoding = self.tokenizer.encode_plus(\n",
        "      tweet,\n",
        "      add_special_tokens=True,\n",
        "      truncation=True,\n",
        "      max_length=512,    # change to 512\n",
        "      return_token_type_ids=False,\n",
        "      pad_to_max_length=True,\n",
        "      return_attention_mask=True,\n",
        "      return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        'input_ids': encoding['input_ids'].flatten(),\n",
        "        'attention_mask': encoding['attention_mask'].flatten(),\n",
        "        'label': torch.tensor(label, dtype=torch.long)\n",
        "    }\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TSQo740tpZs1",
        "outputId": "2c2a732e-2402-4ed0-da8f-0ad150e91254"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f5391275cd0>"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# set random seed for reproducability\n",
        "seed_val = 42\n",
        "torch.manual_seed(seed_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGQA5iJkgfB6"
      },
      "source": [
        "# Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPVQGrLOhcDI",
        "outputId": "7d4aabb7-b331-4a7b-a2a8-f968916f0c44"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/convert_slow_tokenizer.py:454: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "PRETRAINED_MODEL = \"castorini/afriberta_large\"\n",
        "\n",
        "train_data_path = 'afrisent-semeval-2023/data/ibo/train.tsv'\n",
        "test_data_path = 'afrisent-semeval-2023/data/ibo/test.tsv'\n",
        "val_data_path = 'afrisent-semeval-2023/data/ibo/dev.tsv'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BmRxtpLxgnK1"
      },
      "outputs": [],
      "source": [
        "# set up dataset and dataloader\n",
        "train_data = TweetDataset(train_data_path, tokenizer)\n",
        "test_data = TweetDataset(test_data_path, tokenizer)\n",
        "val_data = TweetDataset(val_data_path, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kBMcs2wBkEOf"
      },
      "outputs": [],
      "source": [
        "# define data loaders\n",
        "train_dataloader = DataLoader(train_data, batch_size=16, shuffle=True)\n",
        "val_dataloader = DataLoader(val_data, batch_size=16, shuffle=False)\n",
        "test_dataloader = DataLoader(test_data, batch_size=16, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ir8yTb-wqxqa"
      },
      "source": [
        "# Defining the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iff6jqTOq2wg",
        "outputId": "35b997cb-eacd-4459-dc3a-8e096b0882f3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at castorini/afriberta_large were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias']\n",
            "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at castorini/afriberta_large and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "XLMRobertaForSequenceClassification(\n",
              "  (roberta): XLMRobertaModel(\n",
              "    (embeddings): XLMRobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(70006, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): XLMRobertaEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-9): 10 x XLMRobertaLayer(\n",
              "          (attention): XLMRobertaAttention(\n",
              "            (self): XLMRobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): XLMRobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): XLMRobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): XLMRobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (classifier): XLMRobertaClassificationHead(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# initialize the model\n",
        "model = AutoModelForSequenceClassification.from_pretrained(PRETRAINED_MODEL, num_labels=2)\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VyTn52NTrBPq",
        "outputId": "fc886a93-dafe-4f63-a5a4-e23581f4b1e8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# set optimizer and loss function\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5, correct_bias=False)\n",
        "criterion = torch.nn.BCEWithLogitsLoss()    # alternated between CELoss and BCEWithLogitsLoss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZ4_RWXlrHto",
        "outputId": "a0661004-2198-416a-8218-50fc9638981d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "----------\n",
            "Epoch 1/5, Train Loss: 0.390, Train Acc: 0.830\n",
            "Precision: 0.908, Recall: 0.919, F1: 0.914\n",
            "Epoch 1/5: Train Loss: 0.390, Train Acc: 0.830, Eval Loss: 0.310, Eval Acc: 0.871\n",
            "Epoch 2/5\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/5, Train Loss: 0.221, Train Acc: 0.912\n",
            "Precision: 0.910, Recall: 0.933, F1: 0.921\n",
            "Epoch 2/5: Train Loss: 0.221, Train Acc: 0.912, Eval Loss: 0.301, Eval Acc: 0.881\n",
            "Epoch 3/5\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/5, Train Loss: 0.110, Train Acc: 0.961\n",
            "Precision: 0.917, Recall: 0.924, F1: 0.921\n",
            "Epoch 3/5: Train Loss: 0.110, Train Acc: 0.961, Eval Loss: 0.394, Eval Acc: 0.882\n",
            "Epoch 4/5\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/5, Train Loss: 0.058, Train Acc: 0.982\n",
            "Precision: 0.918, Recall: 0.922, F1: 0.920\n",
            "Epoch 4/5: Train Loss: 0.058, Train Acc: 0.982, Eval Loss: 0.491, Eval Acc: 0.880\n",
            "Epoch 5/5\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/5, Train Loss: 0.029, Train Acc: 0.991\n",
            "Precision: 0.914, Recall: 0.929, F1: 0.921\n",
            "Epoch 5/5: Train Loss: 0.029, Train Acc: 0.991, Eval Loss: 0.619, Eval Acc: 0.882\n"
          ]
        }
      ],
      "source": [
        "# train the model\n",
        "epochs = 5\n",
        "for epoch in range(epochs):\n",
        "    print(f'Epoch {epoch + 1}/{epochs}')\n",
        "    print('-' * 10)\n",
        "\n",
        "    # set model to training mode\n",
        "    model.train()\n",
        "\n",
        "    train_loss, train_acc = 0, 0\n",
        "    total_train_preds = 0\n",
        "    total_train_correct_preds = 0\n",
        "    total_val_preds_list = []\n",
        "    total_val_correct_preds_list = []\n",
        "\n",
        "\n",
        "    for batch in train_dataloader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        loss = outputs.loss    # criterion(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, preds = torch.max(logits, dim=1)\n",
        "        total_train_preds += labels.size(0)\n",
        "        total_train_correct_preds += torch.sum(preds == labels)\n",
        "\n",
        "\n",
        "    train_loss /= len(train_dataloader)\n",
        "    train_acc = total_train_correct_preds.double() / total_train_preds\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc:.3f}\")\n",
        "\n",
        "\n",
        "    # Evaluation loop\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "\n",
        "    eval_loss = 0\n",
        "    eval_acc = 0\n",
        "    total_val_preds = 0\n",
        "    total_val_correct_preds = 0\n",
        "\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in val_dataloader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss     # criterion(outputs, labels)\n",
        "            logits = outputs.logits\n",
        "\n",
        "            eval_loss += loss.item()\n",
        "            _, preds = torch.max(logits, dim=1)\n",
        "            total_val_preds += labels.size(0)\n",
        "            total_val_correct_preds += torch.sum(preds == labels)\n",
        "\n",
        "\n",
        "            predictions.extend(preds.cpu().numpy())\n",
        "            true_labels .extend(labels.cpu().numpy())\n",
        "\n",
        "    eval_loss /= len(val_dataloader)\n",
        "    eval_acc = total_val_correct_preds.double() / total_val_preds\n",
        "\n",
        "    # calculate evaluation metrics\n",
        "    precision = precision_score(true_labels, predictions)\n",
        "    recall = recall_score(true_labels, predictions)\n",
        "    f1 = f1_score(true_labels, predictions)\n",
        "    \n",
        "    print(f\"Precision: {precision:.3f}, Recall: {recall:.3f}, F1: {f1:.3f}\")\n",
        "    print(f\"Epoch {epoch+1}/{epochs}: Train Loss: {train_loss:.3f}, Train Acc: {train_acc:.3f}, Eval Loss: {eval_loss:.3f}, Eval Acc: {eval_acc:.3f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
